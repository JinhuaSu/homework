{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.224.0.11:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BB1657CFD3B4B7208674BEB7D26014B2',\n",
       "  '2820',\n",
       "  'usr',\n",
       "  '2',\n",
       "  '07:03:37',\n",
       "  '2',\n",
       "  '07:05:23',\n",
       "  '106',\n",
       "  '316668',\n",
       "  '47456'],\n",
       " ['BB1657CFD3B4B7208674BEB7D26014B2',\n",
       "  '20858',\n",
       "  'usr',\n",
       "  '2',\n",
       "  '07:05:38',\n",
       "  '2',\n",
       "  '07:11:27',\n",
       "  '348',\n",
       "  '1915724',\n",
       "  '105292'],\n",
       " ['BB1657CFD3B4B7208674BEB7D26014B2',\n",
       "  '15841',\n",
       "  'usr',\n",
       "  '2',\n",
       "  '07:12:18',\n",
       "  '2',\n",
       "  '07:12:20',\n",
       "  '2',\n",
       "  '0',\n",
       "  '0'],\n",
       " ['BB1657CFD3B4B7208674BEB7D26014B2',\n",
       "  '15841',\n",
       "  'usr',\n",
       "  '2',\n",
       "  '07:12:22',\n",
       "  '2',\n",
       "  '07:12:22',\n",
       "  '3',\n",
       "  '0',\n",
       "  '0'],\n",
       " ['BB1657CFD3B4B7208674BEB7D26014B2',\n",
       "  '20858',\n",
       "  'usr',\n",
       "  '2',\n",
       "  '07:12:23',\n",
       "  '2',\n",
       "  '07:14:11',\n",
       "  '108',\n",
       "  '61059',\n",
       "  '11193']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days_list = []\n",
    "for i in range(1,31):\n",
    "    day_num = str(i) if len(str(i))==2 else '0'+str(i)\n",
    "    day = sc.textFile('file:///data/mobile/raw_data/day%s.txt' % day_num).map(lambda x:x.split(','))\n",
    "    days_list.append(day)\n",
    "days_list[1].take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BF279627F55747C654F5189DF3A7A25A-6588', 1),\n",
       " ('A01CDC9E3665AB656124CA42286EF364-11088', 1),\n",
       " ('95B156DFF328E67C5AEE11221C6ED56A-16639', 1),\n",
       " ('EDA0269D6ABF7CE6CFAE029CFFB61423-3309', 1),\n",
       " ('FB7B9BA47A5C652338C50409E67EA4C7-20720', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days_ = sc.emptyRDD()\n",
    "for i in range(23,30):\n",
    "    days_ = days_.union(days_list[i].map(lambda x:(x[0]+'-'+x[1],1)))\n",
    "y = days_.reduceByKey(lambda x,y:1)\n",
    "y.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BB1657CFD3B4B7208674BEB7D26014B2-18386', (23, 524)),\n",
       " ('BB1657CFD3B4B7208674BEB7D26014B2-16759', (23, 14083)),\n",
       " ('BB1657CFD3B4B7208674BEB7D26014B2-6597', (23, 101)),\n",
       " ('210BE886FAD8E61BC45475DDC7B62670-3309', (23, 14975)),\n",
       " ('214678BE5961F27D2C57F1987C60BAE2-8832', (23, 14))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_days = sc.emptyRDD()\n",
    "for i in range(23):\n",
    "    _days = _days.union(days_list[i].map(lambda x:(x[0]+'-'+x[1],(23-i,int(x[-3])))).reduceByKey(lambda x,y:(x[0],x[1]+y[1])))\n",
    "_days.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('98E26BB54ABC5AEDCDFF20B1971C88FB-18024', (1, 1553)),\n",
       " ('A50F35DDB18ED612C71D692E5FE7524E-11088', (1, 28)),\n",
       " ('0CDC7984639DC3BF0B82E96F81A290E8-11088', (1, 314)),\n",
       " ('73AA0F1EF183333CF6DE14FD9912175B-10686', (1, 607)),\n",
       " ('CA77140BE312992B5547625A22719FE7-5205', (1, 42))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_2 = _days.reduceByKey(lambda x,y:(y[0],y[1]) if y[0]<x[0] else (x[0],x[1]))\n",
    "x1_2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "_days.countByKey?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('98E26BB54ABC5AEDCDFF20B1971C88FB-18024', 0.8695652173913043),\n",
       " ('A50F35DDB18ED612C71D692E5FE7524E-11088', 0.6956521739130435),\n",
       " ('0CDC7984639DC3BF0B82E96F81A290E8-11088', 0.9130434782608695),\n",
       " ('73AA0F1EF183333CF6DE14FD9912175B-10686', 0.8260869565217391),\n",
       " ('CA77140BE312992B5547625A22719FE7-5205', 0.6521739130434783)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 = _days.map(lambda x:(x[0],1)).reduceByKey(lambda x,y:x+y).map(lambda x:(x[0],x[1]/23))\n",
    "x3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('98E26BB54ABC5AEDCDFF20B1971C88FB-18024', 1543.0434782608695),\n",
       " ('A50F35DDB18ED612C71D692E5FE7524E-11088', 66.3913043478261),\n",
       " ('0CDC7984639DC3BF0B82E96F81A290E8-11088', 194.47826086956522),\n",
       " ('73AA0F1EF183333CF6DE14FD9912175B-10686', 358.4782608695652),\n",
       " ('CA77140BE312992B5547625A22719FE7-5205', 47.82608695652174)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4 = _days.reduceByKey(lambda x,y:(0,x[1]+y[1]))\\\n",
    ".map(lambda x:(x[0],x[1][1]/23))\n",
    "x4.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('98E26BB54ABC5AEDCDFF20B1971C88FB-18024', 1774.5),\n",
       " ('A50F35DDB18ED612C71D692E5FE7524E-11088', 95.4375),\n",
       " ('0CDC7984639DC3BF0B82E96F81A290E8-11088', 213.0),\n",
       " ('73AA0F1EF183333CF6DE14FD9912175B-10686', 433.94736842105266),\n",
       " ('CA77140BE312992B5547625A22719FE7-5205', 73.33333333333333)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x5 = _days.map(lambda x:(x[0],(1,x[1][1]))).reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1]))\\\n",
    ".map(lambda x:(x[0],x[1][1]/x[1][0]))\n",
    "x5.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C504A4C2350DCCC09D89077D68BA1169-20761', 614),\n",
       " ('C0D7EC409096A42DC3754D2EEC372A46-13043', 137),\n",
       " ('C0D7EC409096A42DC3754D2EEC372A46-20720', 29),\n",
       " ('9AEF49D425761C04EC656E26BFF64153-3309', 1617),\n",
       " ('B0AB7168E0057CE0D64C180A3CE483FB-20761', 18626)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x6 = _days.filter(lambda x:x[1][0]==1).map(lambda x:(x[0],x[1][1]))\n",
    "x6.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5438B4D7AD30A6E66E7C19575E1B88C5-21820', 15528.666666666666),\n",
       " ('3AFCA61EC923AF6FE390B247420269B3-5959', 1148.8333333333333),\n",
       " ('61305E6A64E6DE1CB3A3D78FAA714FBC-9098', 812.0),\n",
       " ('A81CB2210A4DE66EDB450E4AFECB9F0D-10432', 268.5),\n",
       " ('D9FAAEF69375838B39C5C23E5F917620-8165', 2008.6)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x7 = _days.filter(lambda x:x[1][0]<=7).map(lambda x:(x[0],(1,x[1][1]))).reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1]))\\\n",
    ".map(lambda x:(x[0],x[1][1]/x[1][0]))\n",
    "x7.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('98E26BB54ABC5AEDCDFF20B1971C88FB-18024', 1860.1333333333334),\n",
       " ('A50F35DDB18ED612C71D692E5FE7524E-11088', 75.3),\n",
       " ('0CDC7984639DC3BF0B82E96F81A290E8-11088', 238.14285714285714),\n",
       " ('73AA0F1EF183333CF6DE14FD9912175B-10686', 401.84615384615387),\n",
       " ('CA77140BE312992B5547625A22719FE7-5205', 79.41666666666667)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x8 = _days.filter(lambda x:x[1][0]>7).map(lambda x:(x[0],(1,x[1][1]))).reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1]))\\\n",
    ".map(lambda x:(x[0],x[1][1]/x[1][0]))\n",
    "x8.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', (('Spark', 'beijing'), 'Spark')),\n",
       " ('2', (('Hadoop', 'shanghai'), 'shanghai'))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([(\"1\", (\"Spark\", \"beijing\")), (\"2\", (\"Hadoop\", \"shanghai\")),(\"4\", (\"Hadoop\", \"shanghai\"))])\n",
    "rdd2 = sc.parallelize([(\"1\", \"Spark\"), (\"2\",  \"shanghai\"),(\"5\", (\"Hadoop\", \"shanghai\"))])\n",
    "rdd1.join(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.join?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '4']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.keys().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['98E26BB54ABC5AEDCDFF20B1971C88FB-18024',\n",
       " 'A50F35DDB18ED612C71D692E5FE7524E-11088',\n",
       " '0CDC7984639DC3BF0B82E96F81A290E8-11088',\n",
       " '73AA0F1EF183333CF6DE14FD9912175B-10686',\n",
       " 'CA77140BE312992B5547625A22719FE7-5205']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_keys = x4.keys().collect()\n",
    "table_keys[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "collect done\n",
      "1118655\n",
      "for done\n",
      "parallelize done\n",
      "union done\n",
      "done\n",
      "x1_2\n",
      "collect done\n",
      "1741472\n",
      "for done\n",
      "parallelize done\n",
      "union done\n",
      "done\n",
      "x3\n",
      "collect done\n",
      "1741472\n",
      "for done\n",
      "parallelize done\n",
      "union done\n",
      "done\n",
      "x5\n",
      "collect done\n",
      "1741472\n",
      "for done\n",
      "parallelize done\n",
      "union done\n",
      "done\n",
      "x6\n",
      "collect done\n",
      "478314\n",
      "for done\n",
      "parallelize done\n",
      "union done\n",
      "done\n",
      "x7\n",
      "collect done\n",
      "1083785\n",
      "for done\n",
      "parallelize done\n",
      "union done\n",
      "done\n",
      "x8\n",
      "collect done\n",
      "1541854\n",
      "for done\n",
      "parallelize done\n",
      "union done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "rdd_dist = {'y':y,'x1_2':x1_2,'x3':x3,'x5':x5,'x6':x6,'x7':x7,'x8':x8}\n",
    "miss_list_dist = {'y':[],'x1_2':[],'x3':[],'x5':[],'x6':[],'x7':[],'x8':[]}\n",
    "null_dist = {'y':0,'x1_2':(24,0),'x3':0,'x5':0,'x6':0,'x7':0,'x8':0}\n",
    "new_dict = {}\n",
    "table_keys = set(table_keys)\n",
    "for str_,rdd in rdd_dist.items():\n",
    "    print(str_)\n",
    "    tmp_keys = set(rdd.keys().collect())\n",
    "    print('collect done')\n",
    "    print(len(tmp_keys))\n",
    "#     for key in table_keys:\n",
    "#         if key not in tmp_keys:\n",
    "#             miss_list_dist[str_].append(key)\n",
    "    miss_list_dist[str_] = table_keys - tmp_keys\n",
    "    print('for done')\n",
    "    tmp_rdd = sc.parallelize([(key,null_dist[str_]) for key in miss_list_dist[str_]])\n",
    "    print('parallelize done')\n",
    "    new_rdd = rdd.union(tmp_rdd)\n",
    "    print('union done')\n",
    "    new_dict[str_] = new_rdd\n",
    "    print('done')\n",
    "new_dict['x4'] = x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('044E7597FF48E6B7502A702E3DABDB21-4983',\n",
       "  (((((((0, (8, 83)), 0.08695652173913043), 213.0), 0), 0), 213.0),\n",
       "   18.52173913043478)),\n",
       " ('B9618CA6127006B80ACC088BA5A91C68-13983',\n",
       "  (((((((0, (2, 77)), 0.043478260869565216), 77.0), 0), 77.0), 0),\n",
       "   3.347826086956522)),\n",
       " ('5A9034ADADAF1A0F70D4DDDDD440E9D0-21004',\n",
       "  (((((((0, (18, 825)), 0.08695652173913043), 841.0), 0), 0), 841.0),\n",
       "   73.1304347826087)),\n",
       " ('66DEB41FFE7BE7A3643E8C3208FAB759-11088',\n",
       "  (((((((1, (7, 42)), 0.043478260869565216), 42.0), 0), 42.0), 0),\n",
       "   1.826086956521739)),\n",
       " ('77E68627E40041D3B64A255D4D201EFE-3427',\n",
       "  (((((((1, (6, 43)), 0.08695652173913043), 25.0), 0), 43.0), 7.0),\n",
       "   2.1739130434782608))]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rdd = sc.emptyRDD().union(new_dict['y'])\n",
    "for str_,rdd in new_dict.items():\n",
    "    if str_ != 'y':\n",
    "        final_rdd = final_rdd.join(rdd)\n",
    "final_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('044E7597FF48E6B7502A702E3DABDB21-4983',\n",
       "  0,\n",
       "  8,\n",
       "  83,\n",
       "  0.08695652173913043,\n",
       "  213.0,\n",
       "  0,\n",
       "  0,\n",
       "  213.0,\n",
       "  18.52173913043478),\n",
       " ('B9618CA6127006B80ACC088BA5A91C68-13983',\n",
       "  0,\n",
       "  2,\n",
       "  77,\n",
       "  0.043478260869565216,\n",
       "  77.0,\n",
       "  0,\n",
       "  77.0,\n",
       "  0,\n",
       "  3.347826086956522),\n",
       " ('5A9034ADADAF1A0F70D4DDDDD440E9D0-21004',\n",
       "  0,\n",
       "  18,\n",
       "  825,\n",
       "  0.08695652173913043,\n",
       "  841.0,\n",
       "  0,\n",
       "  0,\n",
       "  841.0,\n",
       "  73.1304347826087),\n",
       " ('66DEB41FFE7BE7A3643E8C3208FAB759-11088',\n",
       "  1,\n",
       "  7,\n",
       "  42,\n",
       "  0.043478260869565216,\n",
       "  42.0,\n",
       "  0,\n",
       "  42.0,\n",
       "  0,\n",
       "  1.826086956521739),\n",
       " ('77E68627E40041D3B64A255D4D201EFE-3427',\n",
       "  1,\n",
       "  6,\n",
       "  43,\n",
       "  0.08695652173913043,\n",
       "  25.0,\n",
       "  0,\n",
       "  43.0,\n",
       "  7.0,\n",
       "  2.1739130434782608)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_final = final_rdd.map(lambda x:(x[0],x[1][0][0][0][0][0][0][0],x[1][0][0][0][0][0][0][1][0]\\\n",
    "                                    ,x[1][0][0][0][0][0][0][1][1],x[1][0][0][0][0][0][1],x[1][0][0][0][0][1],x[1][0][0][0][1]\\\n",
    "                                    ,x[1][0][0][1],x[1][0][1],x[1][1]))\n",
    "real_final.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/mobile/processed_data/yx1_8.txt','w') as f:\n",
    "    for tu in real_final.collect():\n",
    "        f.write(','.join([str(item) for item in tu])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [8.0,83.0,0.08695652173913043,213.0,0.0,0.0,213.0,18.52173913043478]),\n",
       " LabeledPoint(0.0, [2.0,77.0,0.043478260869565216,77.0,0.0,77.0,0.0,3.347826086956522]),\n",
       " LabeledPoint(0.0, [18.0,825.0,0.08695652173913043,841.0,0.0,0.0,841.0,73.1304347826087]),\n",
       " LabeledPoint(1.0, [7.0,42.0,0.043478260869565216,42.0,0.0,42.0,0.0,1.826086956521739]),\n",
       " LabeledPoint(1.0, [6.0,43.0,0.08695652173913043,25.0,0.0,43.0,7.0,2.1739130434782608])]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_training = real_final.map(lambda x: LabeledPoint(float(x[1]),[float(x[i]) for i in range(2,10)]))\n",
    "for_training.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本个数：1393518测试集样本个数：347954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1660] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(trainData, testData) = for_training.randomSplit([8,2])\n",
    "print(\"训练集样本个数：\"+str(trainData.count()) + \"测试集样本个数：\"+str(testData.count()))\n",
    "\n",
    "# 将数据暂存在内存中，加快后续运算效率\n",
    "trainData.persist()\n",
    "testData.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-8f894f19f046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlrm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionWithSGD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mintercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/download/spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, data, iterations, step, miniBatchFraction, initialWeights, regParam, regType, intercept, validateData, convergenceTol)\u001b[0m\n\u001b[1;32m    323\u001b[0m                                  bool(intercept), bool(validateData), float(convergenceTol))\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/download/spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/regression.py\u001b[0m in \u001b[0;36m_regression_train_wrapper\u001b[0;34m(train_func, modelClass, data, initial_weights)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodelClass\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         weights, intercept, numFeatures, numClasses = train_func(\n\u001b[0;32m--> 217\u001b[0;31m             data, _convert_to_vector(initial_weights))\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodelClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/download/spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rdd, i)\u001b[0m\n\u001b[1;32m    321\u001b[0m             return callMLlibFunc(\"trainLogisticRegressionModelWithSGD\", rdd, int(iterations),\n\u001b[1;32m    322\u001b[0m                                  \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminiBatchFraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                                  bool(intercept), bool(validateData), float(convergenceTol))\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/download/spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/download/spark-2.4.0-bin-hadoop2.7/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/download/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/download/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/download/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd/bqw/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rf = RandomForest.trainClassifier(sc.parallelize(trainData), 2, {}, 3, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: 0.0\n",
      "coeffs: [-6.599803309256028,-850.9659640096515,0.21681646603047455,-7.353918733245337,7531.449445688068,2630.158661119206,4009.327432071751,6207.529855853209]\n"
     ]
    }
   ],
   "source": [
    "print(f'intercept: {lrm.intercept}')\n",
    "print(f'coeffs: {lrm.weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [2.0,77.0,0.043478260869565216,77.0,0.0,77.0,0.0,3.347826086956522]),\n",
       " LabeledPoint(1.0, [1.0,441.0,0.782608695652174,372.5,441.0,230.0,413.2142857142857,291.5217391304348]),\n",
       " LabeledPoint(0.0, [5.0,1138.0,0.08695652173913043,1952.0,0.0,1138.0,2766.0,169.7391304347826]),\n",
       " LabeledPoint(0.0, [1.0,39.0,0.043478260869565216,39.0,39.0,39.0,0.0,1.6956521739130435]),\n",
       " LabeledPoint(1.0, [2.0,8.0,0.391304347826087,61.888888888888886,0.0,15.666666666666666,85.0,24.217391304347824])]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = testData.take(5)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([2.0, 77.0, 0.0435, 77.0, 0.0, 77.0, 0.0, 3.3478])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.clearThreshold()\n",
    "lrm.predict(test_data[0].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "score_label_lr = testData.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109075.50646850988"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVMWithSGD.train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = SVMWithSGD.train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
